name: Step-1 & Step-2 E2E Validation

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  e2e-test:
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        working-directory: ai-runtime
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]

      - name: Run Step-2 unit tests
        working-directory: ai-runtime
        env:
          USE_MOCK_MODEL: "true"
        run: |
          pytest server/tests/test_batcher.py server/tests/test_scheduler.py server/tests/test_queue.py server/tests/test_concurrency.py -v

      - name: Run Step-4 gateway tests
        working-directory: ai-runtime
        env:
          ENABLE_STREAMING: "true"
          API_KEYS: "tenant1:test-key-1,tenant2:test-key-2"
          TENANT_RATE_LIMIT: "100"
          GATEWAY_TIMEOUT_MS: "30000"
          MAX_RETRIES: "1"
        run: |
          pytest gateway/tests/test_streaming.py gateway/tests/test_rate_limiting.py gateway/tests/test_auth.py gateway/tests/test_failures.py -v

      - name: Start server in background
        working-directory: ai-runtime
        env:
          USE_MOCK_MODEL: "true"
          BATCH_MAX_SIZE: "4"
          BATCH_MAX_LATENCY_MS: "50"
          MAX_IN_FLIGHT_REQUESTS: "50"
        run: |
          uvicorn server.app.main:app --host 127.0.0.1 --port 8000 &
          echo $! > /tmp/server.pid

      - name: Wait for server health
        run: |
          for i in {1..30}; do
            if curl -f http://127.0.0.1:8000/health > /dev/null 2>&1; then
              echo "Server is healthy"
              exit 0
            fi
            sleep 1
          done
          echo "Server failed to start"
          exit 1

      - name: Run smoke test
        working-directory: ai-runtime
        run: |
          python scripts/smoke_test.py

      - name: Test Step-2 batching and multi-GPU
        working-directory: ai-runtime
        run: |
          # Test that server handles concurrent requests (batching)
          python -c "
          import requests
          import concurrent.futures
          import time
          
          def make_request(i):
              try:
                  resp = requests.post('http://127.0.0.1:8000/infer', 
                                     json={'prompt': f'test {i}', 'api_version': 'v1'},
                                     timeout=10)
                  return resp.status_code == 200
              except Exception as e:
                  print(f'Request {i} failed: {e}')
                  return False
          
          # Send 10 concurrent requests to test batching
          with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
              futures = [executor.submit(make_request, i) for i in range(10)]
              results = [f.result() for f in concurrent.futures.as_completed(futures)]
          
          success_count = sum(results)
          print(f'Successfully processed {success_count}/10 requests')
          assert success_count >= 8, f'Expected at least 8 successful requests, got {success_count}'
          "

      - name: Stop server
        if: always()
        run: |
          if [ -f /tmp/server.pid ]; then
            kill $(cat /tmp/server.pid) || true
          fi

